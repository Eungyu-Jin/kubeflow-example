{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp.components import create_component_from_func\n",
    "\n",
    "BASE_IMAGE = \"easyjin/penguins-example:0.1\"\n",
    "\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"palmerpenguins\"]\n",
    ")\n",
    "def dataprep():\n",
    "    from palmerpenguins import load_penguins\n",
    "    import pandas as pd\n",
    "    raw_ds = load_penguins()\n",
    "    raw_ds.dropna(axis=0, inplace=True)\n",
    "    raw_ds.to_csv('/mnt/penguins-data.csv')\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_ds, test_ds = train_test_split(raw_ds, test_size=0.2, random_state=47)\n",
    "\n",
    "    train_ds.to_csv('/mnt/train-ds.csv', index=False)\n",
    "    test_ds.to_csv('/mnt/test-ds.csv', index=False)\n",
    "    \n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def trainer(\n",
    "    criterion: str,\n",
    "    n_estimators: int,\n",
    "    max_depth: int,\n",
    "    model_path: OutputPath(\"dill\"),\n",
    "):\n",
    "    import dill\n",
    "    import pandas as pd\n",
    "    \n",
    "    def base(criterion, n_estimators, max_depth):\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "        # numerical\n",
    "        nums = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "        num_tfm = Pipeline(\n",
    "            steps=[(\"scaler\", StandardScaler())]\n",
    "        )\n",
    "        # categorical\n",
    "        cats = [\"island\", \"sex\"]\n",
    "        cat_tfm = Pipeline(\n",
    "            steps=[\n",
    "                (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "            ]\n",
    "        )\n",
    "        # column merge\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_tfm, nums),\n",
    "                (\"cat\", cat_tfm, cats),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        base_model = Pipeline(\n",
    "            steps=[\n",
    "                (\"prep\", preprocessor),\n",
    "                (\"rf\", RandomForestClassifier(\n",
    "                        criterion= criterion,\n",
    "                        n_estimators= n_estimators,\n",
    "                        max_depth= max_depth,\n",
    "                        oob_score=True,\n",
    "                        n_jobs = -1\n",
    "                    ))\n",
    "            ]\n",
    "        )\n",
    "        return base_model\n",
    "\n",
    "    model = base(criterion, n_estimators, max_depth)\n",
    "\n",
    "    train_ds = pd.read_csv('/mnt/train-ds.csv')\n",
    "    X_train = train_ds.iloc[:,1:]\n",
    "    y_train = train_ds.iloc[:,0].values.ravel()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # save model\n",
    "    with open(model_path, 'wb') as f:\n",
    "        dill.dump(model, f)\n",
    "\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def metric(\n",
    "    model_path: InputPath(\"dill\"),\n",
    "    # mlpipeline_ui_metadata_path: OutputPath(\"UI_Metadata\"),\n",
    "    mlpipeline_metrics_path: OutputPath(\"Metrics\")\n",
    "):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import dill\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    test_ds = pd.read_csv('/mnt/train-ds.csv')\n",
    "    X_test = test_ds.iloc[:,1:]\n",
    "    y_test = test_ds.iloc[:,0].values.ravel()\n",
    "\n",
    "    with open(model_path, mode=\"rb\") as f:\n",
    "        model = dill.load(f)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        'metrics': [\n",
    "            {\n",
    "                'name': 'oob-score',\n",
    "                'numberValue':  model['rf'].oob_score_,\n",
    "                'format': 'RAW'\n",
    "            },\n",
    "            {\n",
    "                'name': 'accuracy-score',\n",
    "                'numberValue':  accuracy_score(y_pred, y_test),\n",
    "                'format': 'RAW'       \n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "        \n",
    "\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def stack_mlflow(\n",
    "    model_name: str,\n",
    "    model_path: InputPath(\"dill\"),\n",
    "):\n",
    "    import os\n",
    "    import dill\n",
    "    import pandas as pd\n",
    "\n",
    "    from mlflow.sklearn import save_model\n",
    "    from mlflow.tracking.client import MlflowClient\n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n",
    "\n",
    "    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n",
    "\n",
    "    train_ds = pd.read_csv('/mnt/train-ds.csv')\n",
    "    X_train = train_ds.iloc[:,1:]\n",
    "\n",
    "    with open(model_path, mode=\"rb\") as f:\n",
    "        model = dill.load(f)\n",
    "\n",
    "    from mlflow.models.signature import infer_signature\n",
    "    input_example = X_train.sample(1)\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    save_model(\n",
    "        sk_model=model,\n",
    "        path=model_name,\n",
    "        serialization_format=\"cloudpickle\",\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "    )\n",
    "    run = client.create_run(experiment_id=\"0\")\n",
    "    client.log_artifact(run.info.run_id, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import onprem\n",
    "\n",
    "def trainer_func(criterion, n_estimators, max_depth):\n",
    "    return dsl.ContainerOp(\n",
    "        name=\"training model\",\n",
    "        image=\"192.168.0.50:5100/penguins-trainer:0.1\",\n",
    "        command=[\"python\", \"trainer.py\"],\n",
    "        arguments=[\n",
    "            \"--criterion\", criterion,\n",
    "            \"--n_estimators\", n_estimators,\n",
    "            \"--max_depth\", max_depth,\n",
    "            \"--train_ds_path\", '/mnt/train-ds.csv',\n",
    "            ],\n",
    "        file_outputs={\"model\": \"model.pkl\"}\n",
    "    )\n",
    "\n",
    "@dsl.pipeline(name=\"penguins-pipeline\")\n",
    "def penguins_pipeline(criterion:str = 'gini', n_estimators: int = 300, max_depth: int = 3, model_name: str = 'penguins-clf'):\n",
    "    data_mnt = onprem.mount_pvc(pvc_name='pvc-penguins', volume_name= 'data-mnt',volume_mount_path='/mnt')\n",
    "    \n",
    "    dataprep_op = dataprep().apply(data_mnt)\n",
    "    \n",
    "    trainer_op = trainer(criterion, n_estimators, max_depth)\n",
    "    # trainer_op = trainer_func(criterion, n_estimators, max_depth)\n",
    "    trainer_op.apply(data_mnt)\n",
    "    trainer_op.after(dataprep_op)\n",
    "\n",
    "    metric(\n",
    "        model = trainer_op.outputs[\"model\"]       \n",
    "    ).apply(data_mnt)\n",
    "    \n",
    "    stack_mlflow(\n",
    "        model_name=model_name,\n",
    "        model=trainer_op.outputs[\"model\"],\n",
    "    ).apply(data_mnt)\n",
    "\n",
    "from kfp import compiler\n",
    "compiler.Compiler().compile(penguins_pipeline, 'penguins_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "USERNAME = \"user@example.com\"\n",
    "PASSWORD = \"12341234\"\n",
    "NAMESPACE = \"kubeflow-user-example-com\"\n",
    "HOST = \"http://localhost:30398\"\n",
    "\n",
    "session = requests.Session()\n",
    "response = session.get(HOST)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "}\n",
    "\n",
    "data = {\"login\": USERNAME, \"password\": PASSWORD}\n",
    "session.post(response.url, headers=headers, data=data)\n",
    "\n",
    "session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "\n",
    "import kfp\n",
    "client = kfp.Client(\n",
    "    host=f\"{HOST}/pipeline\", \n",
    "    namespace=NAMESPACE, \n",
    "    cookies=f\"authservice_session={session_cookie}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.0.50:30398/pipeline/#/experiments/details/1e8cf05b-3575-4238-88f4-630247c093e1\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.0.50:30398/pipeline/#/runs/details/67d5218f-5e40-40cd-91fe-2e262696da40\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=67d5218f-5e40-40cd-91fe-2e262696da40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_NAME = \"examples\"\n",
    "\n",
    "arguments = {'criterion':'entropy' ,'n_estimators': 172, 'max_depth': 4, 'model_name': 'penguins-clf'}\n",
    "client.create_run_from_pipeline_func(penguins_pipeline, experiment_name = EXP_NAME, arguments=arguments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### katib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "\n",
    "from kubeflow.katib import ApiClient\n",
    "from kubeflow.katib import V1beta1ExperimentSpec\n",
    "from kubeflow.katib import V1beta1AlgorithmSpec\n",
    "from kubeflow.katib import V1beta1EarlyStoppingSpec\n",
    "from kubeflow.katib import V1beta1EarlyStoppingSetting\n",
    "from kubeflow.katib import V1beta1ObjectiveSpec\n",
    "from kubeflow.katib import V1beta1ParameterSpec\n",
    "from kubeflow.katib import V1beta1FeasibleSpace\n",
    "from kubeflow.katib import V1beta1TrialTemplate\n",
    "from kubeflow.katib import V1beta1TrialParameterSpec\n",
    "from kubeflow.katib import V1beta1MetricsCollectorSpec\n",
    "\n",
    "EXP_NAME = \"examples\"\n",
    "NAMESPANCE = \"kubeflow-user-example-com\"\n",
    "\n",
    "max_trial_count = 10\n",
    "max_failed_trial_count = 3\n",
    "parallel_trial_count = 2\n",
    "\n",
    "objective=V1beta1ObjectiveSpec(\n",
    "    type=\"maximize\",\n",
    "    # goal= 0.99,\n",
    "    objective_metric_name=\"accuracy\"\n",
    ")\n",
    "\n",
    "algorithm=V1beta1AlgorithmSpec(\n",
    "    algorithm_name=\"bayesianoptimization\",\n",
    ")\n",
    "\n",
    "early_stopping=V1beta1EarlyStoppingSpec(\n",
    "    algorithm_name=\"medianstop\",\n",
    "    algorithm_settings=[\n",
    "        V1beta1EarlyStoppingSetting(\n",
    "            name=\"min_trials_required\",\n",
    "            value=\"2\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# parametersSpec\n",
    "parameters=[\n",
    "    V1beta1ParameterSpec(\n",
    "        name=\"n_estimators\",\n",
    "        parameter_type=\"int\",\n",
    "        feasible_space=V1beta1FeasibleSpace(\n",
    "            min=\"100\",\n",
    "            max=\"1000\",\n",
    "            step=\"100\"\n",
    "        ),\n",
    "    ),\n",
    "    V1beta1ParameterSpec(\n",
    "        name=\"max_depth\",\n",
    "        parameter_type=\"int\",\n",
    "        feasible_space=V1beta1FeasibleSpace(\n",
    "            min=\"2\",\n",
    "            max=\"6\",\n",
    "            step=\"1\"\n",
    "        ),\n",
    "    ),\n",
    "    V1beta1ParameterSpec(\n",
    "        name=\"criterion\",\n",
    "        parameter_type=\"categorical\",\n",
    "        feasible_space=V1beta1FeasibleSpace(\n",
    "            list=[\n",
    "                \"gini\", \n",
    "                \"entropy\",\n",
    "                \"log_loss\"\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_spec={\n",
    "    \"apiVersion\": \"batch/v1\",\n",
    "    \"kind\": \"Job\",\n",
    "    \"spec\": {\n",
    "        \"template\": {\n",
    "            \"metadata\": {\n",
    "                \"annotations\": {\n",
    "                     \"sidecar.istio.io/inject\": \"false\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        \"name\": \"training-container\",\n",
    "                        \"image\": \"easyjin/penguins-trainer:0.1\",\n",
    "                        \"command\": [\n",
    "                            \"python\",\n",
    "                            \"trainer.py\",\n",
    "                            \"--criterion=${trialParameters.criterion}\",\n",
    "                            \"--n_estimators=${trialParameters.nEstimators}\",\n",
    "                            \"--max_depth=${trialParameters.maxDepth}\",\n",
    "                            \"--train_ds_path=/mnt/train-ds.csv\"\n",
    "                        ],\n",
    "                        \"volumeMounts\": [\n",
    "                            {\n",
    "                                \"mountPath\": \"/mnt\",\n",
    "                                \"name\": \"katib-mnt\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"volumes\": [\n",
    "                    {\n",
    "                        \"name\": \"katib-mnt\",\n",
    "                        \"persistentVolumeClaim\": {\n",
    "                            \"claimName\": \"pvc-penguins\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"restartPolicy\": \"Never\",\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "trial_template=V1beta1TrialTemplate(\n",
    "    retain=True,\n",
    "    primary_container_name=\"training-container\",\n",
    "    trial_parameters=[\n",
    "        V1beta1TrialParameterSpec(\n",
    "            name=\"criterion\",\n",
    "            description=\"criterion for the training model\",\n",
    "            reference=\"criterion\"\n",
    "        ),\n",
    "        V1beta1TrialParameterSpec(\n",
    "            name=\"nEstimators\",\n",
    "            description=\"Number of training model estimators\",\n",
    "            reference=\"n_estimators\"\n",
    "        ),\n",
    "        V1beta1TrialParameterSpec(\n",
    "            name=\"maxDepth\",\n",
    "            description=\"Training model maxDepth\",\n",
    "            reference=\"max_depth\"\n",
    "        ),\n",
    "    ],\n",
    "    trial_spec=trial_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_spec=V1beta1ExperimentSpec(\n",
    "    max_trial_count=max_trial_count,\n",
    "    max_failed_trial_count=max_failed_trial_count,\n",
    "    parallel_trial_count=parallel_trial_count,\n",
    "    objective=objective,\n",
    "    algorithm=algorithm,\n",
    "    early_stopping=early_stopping,\n",
    "    parameters=parameters,\n",
    "    trial_template=trial_template\n",
    ")\n",
    "\n",
    "katib_experiment_launcher_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/katib-launcher/component.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_katib_results(katib_results):\n",
    "    import json\n",
    "    katib_res = json.loads(katib_results)\n",
    "\n",
    "    convert_res = {}\n",
    "    for p in katib_res['currentOptimalTrial'][\"parameterAssignments\"]:\n",
    "        convert_res[p['name']] = p['value']\n",
    "\n",
    "    with open('mnt/hyps.json', 'w') as f:\n",
    "        json.dump(convert_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import onprem\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"Launch Katib Experiment\",\n",
    "    description=\"An example to launch Katib Experiment with early stopping\"\n",
    ")\n",
    "def penguins_hyps():\n",
    "    katib_op = katib_experiment_launcher_op(\n",
    "        experiment_name=EXP_NAME,\n",
    "        experiment_namespace=NAMESPACE,\n",
    "        experiment_spec=ApiClient().sanitize_for_serialization(experiment_spec),\n",
    "        experiment_timeout_minutes=10,\n",
    "        delete_finished_experiment=False)\n",
    "    \n",
    "    convert_katib_results_op = components.func_to_container_op(convert_katib_results)\n",
    "    best_hp_op = convert_katib_results_op(katib_op.output)\n",
    "    best_hp_op.apply(onprem.mount_pvc(pvc_name='pvc-penguins', volume_name= 'data-mnt',volume_mount_path='/mnt'))\n",
    "        \n",
    "    op_out = dsl.ContainerOp(\n",
    "        name=\"best-hp\",\n",
    "        image=\"library/bash:4.4.23\",\n",
    "        command=[\"sh\", \"-c\"],\n",
    "        arguments=[\"echo Best HyperParameters: %s\" % katib_op.output],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.0.50:30398/pipeline/#/experiments/details/1e8cf05b-3575-4238-88f4-630247c093e1\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://192.168.0.50:30398/pipeline/#/runs/details/610fb159-6e9f-423a-9a8f-5f667428c71a\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=610fb159-6e9f-423a-9a8f-5f667428c71a)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(penguins_hyps, experiment_name = EXP_NAME, arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/mnt/nfsroot/pv-penguins/hyps.json') as f:\n",
    "    best_hyp = json.load(f)\n",
    "\n",
    "best_param = {}\n",
    "for p in best_hyp['currentOptimalTrial'][\"parameterAssignments\"]:\n",
    "    best_param[p['name']] = p['value']\n",
    "\n",
    "print(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow import katib\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": katib.search.int(min=100, max=1000, step=100),\n",
    "    \"max_depth\": katib.search.int(min=2, max=6, step=1),\n",
    "    \"criterion\": katib.search.categorical(['gini', 'entropy', 'log_loss'])\n",
    "}\n",
    "\n",
    "\n",
    "NAMESPANCE = 'kubeflow-user-example-com'\n",
    "katib_client = katib.KatibClient()\n",
    "katib_client.tune(\n",
    "    name=EXP_NAME,\n",
    "    namespace=NAMESPANCE,\n",
    "    objective=trainer, \n",
    "    parameters=params, \n",
    "    algorithm_name=\"bayesianoptimization\",\n",
    "    objective_metric_name=\"accuracy\", \n",
    "    # additional_metric_names=[\"loss\"],\n",
    "    max_trial_count=20, \n",
    "    base_image='python:3.8.10-slim',\n",
    "    packages_to_install=['scikit-learn', 'pandas']\n",
    ")\n",
    "status = katib_client.is_experiment_succeeded(EXP_NAME, namespace='easyjin')\n",
    "print(f\"Katib Experiment is Succeeded: {status}\\n\")\n",
    "best_hps = katib_client.get_optimal_hyperparameters(EXP_NAME, namespace='easyjin')\n",
    "katib_client.get_suggestion(EXP_NAME, namespace='easyjin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
